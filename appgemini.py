import sqlite3
from langchain_community.llms import Ollama

# 1. Configura√ß√£o do Banco de Dados (Mem√≥ria Permanente)
conn = sqlite3.connect('memoria_ia.db')
cursor = conn.cursor()
cursor.execute('''CREATE TABLE IF NOT EXISTS rotina 
                  (id INTEGER PRIMARY KEY, informacao TEXT)''')
conn.commit()

# 2. Inicializa√ß√£o do Modelo Local
llm = Ollama(model="llama3")

def salvar_informacao(texto):
    cursor.execute("INSERT INTO rotina (informacao) VALUES (?)", (texto,))
    conn.commit()
    return "‚úÖ Informa√ß√£o guardada na minha mem√≥ria local."

def buscar_memoria():
    cursor.execute("SELECT informacao FROM rotina")
    rows = cursor.fetchall()
    return "\n".join([r[0] for r in rows])

print("ü§ñ IA de Rotina Local: On! (Comandos: 'anote: [texto]' ou apenas sua d√∫vida)")

while True:
    user_input = input("\nVoc√™: ")
    
    if user_input.lower() in ['sair', 'exit']: break

    if user_input.lower().startswith("anote:"):
        info = user_input.replace("anote:", "").strip()
        print(f"IA: {salvar_informacao(info)}")
    else:
        # Recupera tudo que a IA sabe para ela ter contexto
        contexto_memoria = buscar_memoria()
        
        prompt = f"""
        Voc√™ √© um assistente de rotina. Abaixo est√£o os fatos que voc√™ sabe sobre mim:
        {contexto_memoria}
        
        Responda √† seguinte d√∫vida baseando-se apenas nesses fatos: {user_input}
        Se n√£o souber a resposta, diga que ainda n√£o tem essa informa√ß√£o anotada.
        """
        
        resposta = llm.invoke(prompt)
        print(f"IA: {resposta}")