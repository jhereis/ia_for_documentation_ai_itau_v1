from langchain_community.llms import Ollama
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferWindowMemory

# Inicializa o modelo que vocÃª baixou no Ollama
llm = Ollama(model="llama3")

# Criamos uma memÃ³ria para que ela lembre do contexto da conversa atual
memoria = ConversationBufferWindowMemory(k=5) 

print("ðŸ¤– IA de Rotina Local Ativa! (Digite 'sair' para encerrar)")

while True:
    pergunta = input("VocÃª: ")
    if pergunta.lower() == 'sair':
        break

    # Prompt simples para guiar a IA
    contexto = f"""
    VocÃª Ã© um assistente pessoal de rotina. 
    Seu objetivo Ã© ajudar a organizar o dia a dia e tirar dÃºvidas sobre horÃ¡rios.
    Responda de forma curta e objetiva em PortuguÃªs.
    
    Pergunta: {pergunta}
    """

    resposta = llm.invoke(contexto)
    print(f"IA: {resposta}")
